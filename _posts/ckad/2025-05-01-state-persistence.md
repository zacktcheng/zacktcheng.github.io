# State Persistence

### Table of contents:
- [Volumes](#volumes)
- [Storage Classes](#storage-classes)
- [Stateful Sets](#stateful-sets)

## Volumes

Before diving into Persistent Volumes, it's important to understand the concept of volumes in Kubernetes. Similar to Docker, which is transient in nature, Docker containers are designed to exist temporarily, intended to
process data and be destroyed afterwards. When containers are deleted, the data within them is also removed. To preserve the data processed by containers, Docker uses volumes, allowing data to be stored outside the
container. This ensures that data persists even if the container is destroyed, as the data is stored in the attached volume.

In the Kubernetes environment, pods also exhibit a transient nature. When a pod processes data and is subsequently deleted, the data it handled is lost. To prevent this, volumes are attached to pods. These volumes enable
data generated by a pod to persist beyond the pod's lifecycle. For instance, a Kubernetes implementation might involve a single-node cluster with a pod that generates a random number and writes it to a file. Attaching a
volume to this pod allows the data to be stored separately on the host. Even after the pod's deletion, the data persists because it resides in the volume, which is mounted to a directory within the container using the
volume mounts field.

Exploring storage options for volumes reveals that in a single-node cluster, a host path option may be used for storing data directly on the host. However, in multi-node clusters, this approach is discouraged because it
could lead to inconsistencies across nodes without a shared storage solution. Kubernetes offers a variety of storage solutions for this purpose, including NFS, Cluster FS, Flocker, and cloud-based storage like AWS, EBS,
Azure Disk, or Google's Persistent Disk. For example, configuring an AWS Elastic Block Store volume involves replacing the host path with the AWS Elastic Block Store field, specifying the volume ID and file system type to
ensure data storage on AWS EBS, thereby facilitating data persistence across the cluster's infrastructure.

### Persistent Volumes

In the previous discussions about Kubernetes volumes, the configuration for storage was embedded within the pod definition files, requiring users to specify storage details each time they deployed a pod. This approach can
become cumbersome in large environments with numerous users and pods, as each user must configure storage individually for every pod deployment. Making changes to storage configurations would also necessitate updates
across all deployed pods, creating an administrative burden.

To address these challenges, the concept of persistent volumes is introduced, which centralizes storage management. Persistent volumes provide a cluster-wide storage pool configured by an administrator, allowing users
deploying applications on the cluster to use it efficiently. Users can then make use of this shared storage pool by creating persistent volume claims, simplifying the storage provisioning process.

Creating a persistent volume begins with a base template, where the API version is updated, and the kind is set to persistent volume, named appropriately (e.g., PV-vol-one). The specification for access modes is included,
which determines how the volume is mounted on hosts—options range from read-only many and read-write-once to read-write-many modes. The capacity specifies the storage reserved, say one GB in this context. Additionally, the
volume type needs to be defined. For the example, the host path option, which uses local node storage, is employed, though it's not recommended for production environments.

Once the persistent volume configuration is set, the volume is created with commands like kubectl create, and its status can be checked using `kubectl get persistent volumes`. To transition into production scenarios, the host
path option can be replaced with more robust storage solutions, such as AWS Elastic Block Store, ensuring scalability and reliability in the storage infrastructure.

### Persistent Volume Claims

In Kubernetes, persistent volumes and persistent volume claims are distinct but related objects within the namespace. Administrators are responsible for creating persistent volumes, which serve as a storage pool, while
users create persistent volume claims to utilize this storage. Once a claim is made, Kubernetes binds it to an appropriate persistent volume, based on the request specifications and the properties set on the volume. These
properties can include criteria such as capacity, access modes, volume modes, and storage class.

Persistent volume claims have a one-to-one binding with persistent volumes. This means each claim is connected to a single volume, and if a smaller claim matches well with a larger volume—with no better options—Kubernetes
will bind them. This exclusive binding prevents other claims from using any remaining capacity in the volume. If no volumes are available matching the claim, the claim remains in a pending state until new volumes are made
available. Once a compatible volume is added to the cluster, the claim binds automatically.

Creating a persistent volume claim involves starting with a basic template, setting the API version and kind to "persistent volume claim," and specifying requirements such as access modes and storage capacity. For instance,
a claim might request 500 megabytes of storage with read-write access. Upon creation, if existing volumes meet the necessary specifications, the claim binds to an available volume. If the exact requirements aren't met but
an available volume has greater capacity, the binding still occurs.

Managing the lifecycle of persistent volumes and claims involves deciding what happens to the underlying volume when a claim is deleted. By default, the volume is retained, meaning it persists until manually removed by an
administrator. Alternatively, automatic deletion of the volume upon claim deletion can be set to free up storage. A third option is recycling, where the data is erased before the volume is made available for new claims.
This system provides flexibility in handling storage resources within a Kubernetes environment.

### Use PVCs in Pods

Once a PVC is ready to use in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

```
The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.

[K8s Official doc reference](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes)

## Storage Classes

In earlier discussions, we explored the process of creating Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) to claim storage, which can then be used in pod definition files. A typical example involves creating
a PVC from a Google Cloud persistent disk. However, this process mandates prior manual creation of the disk on Google Cloud, followed by manual provisioning of the persistent volume using a definition file matching the
disk
name. This is known as static provisioning. Manually handling storage for each application can be cumbersome and inefficient.

To streamline this process, storage classes were introduced, enabling dynamic provisioning of volumes. A storage class allows the specification of a provisioner, such as Google Storage, which can automatically provision
storage on Google Cloud and attach it to pods when a claim is made. This eliminates the need for manual PV creation, as the storage, along with the PV, is created automatically when the storage class is defined. The API
version for creating a storage class is Storage.k8s.io/v1, with a provisioner like Kubernetes.io/gce-pd specifying Google Cloud.

Incorporating a storage class into the PVC definition allows the PVC to utilize the defined storage class, determining which provisioner to use when creating a new PVC. On execution, the associated storage class provisions
a new disk with the necessary size on GCP, creates the PV, and binds it to the PVC. This automatic process simplifies storage management, reducing manual interventions and ensuring scalability and efficiency.

Different types of provisioners are available, including alternatives such as AWS EBS, Azure file, Azure disk, and more. These provisioners allow passing specific parameters like disk type or replication mode, with
settings varying depending on the provisioner. For Google persistent disks, parameters may include specifying disk type (Standard or SSD) and replication mode (none or regional pd). This flexibility allows the creation of
varied storage classes, such as a silver class with standard disks, a gold class with SSDs, and a platinum class with SSDs and replication. Thus, storage classes enable the segmentation of storage services to meet
differing performance and redundancy needs, allowing users to select the appropriate storage class for their PVCs.

## Stateful Sets

### Why Stateful Sets?

Understanding the need for StatefulSets in Kubernetes begins with examining the necessity of ordered, stable deployments, particularly in scenarios requiring a predefined sequence and persistent identifiers. Prior to
delving into Kubernetes, consider deploying a MySQL database on physical servers, where replication and data synchronization take precedence. To ensure high availability, additional servers are added, but replicating data
from the original to the new servers presents challenges. A common topology for this is a single master-multislave setup, where the master handles writes and the slaves facilitate reads. The master must be configured
first, followed by the slaves through initial data cloning and enabled continuous replication.

Translating this into Kubernetes, deployments traditionally spawn simultaneously, lacking the ability to maintain a specific sequence or uniquely identify pod roles, like master and slaves, due to dynamic naming and IP
allocation. This variability hinders assured sequencing and role designation necessary for replication. Deployments, hence, face limitations in orchestrating such setups, as pod identity can alter upon recreation, making
static configurations unreliable.

StatefulSets address these limitations by enabling ordered pod creation, ensuring that each pod starts sequentially after its predecessor is running and ready, which conveniently aligns with the needs of database
replication setups. Through StatefulSets, pods are assigned deterministic, stable identities using a unique ordinal index that combines with the set name (e.g., MySQL-0, MySQL-1), ensuring predictability in naming. This
allows the master pod to always be recognized as index zero, retaining its address even after restarts, thus resolving the identity persistence issue.

Moreover, StatefulSets facilitate continuous replication without interruptions in identity, as slaves can always reference the static master address. This feature allows new pods, like a newly scaled MySQL-3, to find its
replication source reliably, aligning with predefined replication steps. Consequently, StatefulSets provide a robust framework for applications necessitating stable, ordered deployments and persistent networking identity.
This makes them vital for applications like MySQL clusters, where maintaining order and identity directly influences performance and reliability, distinctly differing from general deployment strategies.

### Stateful Sets Overview

The necessity of using a StatefulSet in Kubernetes depends largely on the specific nature and requirements of the application being deployed. If your application instances need to start in a particular sequence or require
stable names, then a StatefulSet may be the appropriate choice. Once you determine that a StatefulSet is suitable, creating one involves a process similar to deploying a standard deployment. You begin with a deployment
definition file containing a pod template, then adjust the type from "Deployment" to "StatefulSet" (with both "SS" capitalized).

A unique requirement of StatefulSets is the need for a headless service name. Though detailed discussion on headless services is reserved for later, they play a crucial role in enabling the benefits of StatefulSets. One
primary advantage is that a StatefulSet provisions pods sequentially, ensuring ordered and controlled deployment. Each pod receives a consistent, unique DNS entry, allowing reliable access by other applications. Scaling a
StatefulSet maintains this order, which is particularly useful for applications like MySQL databases, where newer instances clone data from previously deployed instances, ensuring continuity and consistency.

StatefulSets also handle scaling down or deletion systematically, removing or terminating pods in reverse order, maintaining operational stability. However, the ordered approach can be modified if desired. By adjusting the
pod management policy to "parallel," the StatefulSet can deploy pods simultaneously, while still retaining stable and unique network identities. This flexible configuration ensures that the specific deployment needs of
various applications can be met, providing a structured yet adaptable deployment strategy.
